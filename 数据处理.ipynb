{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"data\\main\\df_train_b1.csv\", index_col=0)\n",
    "test_data = pd.read_csv('data\\main\\df_test_b1.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要读入除了main之外的数在此处添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = [\"data\\detail\\df_basic_b1.csv\",\n",
    "             \"data\\detail\\df_corp_b1.csv\",\n",
    "             \"data\\detail\\df_judicial_b1.csv\",\n",
    "             \"data\\detail\\df_loan2_b1.csv\",\n",
    "             \"data\\detail\\df_query_b1.csv\"]\n",
    "for filename in filenames:\n",
    "    temp = pd.read_csv(filename)\n",
    "    train_data = pd.merge(train_data, temp, on=\"cust_id\",\n",
    "                          how=\"left\", suffixes=(\"\", \"_y\"))\n",
    "    test_data = pd.merge(test_data, temp, on=\"cust_id\",\n",
    "                         how=\"left\", suffixes=(\"\", \"_y\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['is_judicial']=[0 if pd.isna(i) else 1 for i in train_data['judicial_reason']]\n",
    "test_data['is_judicial']=[0 if pd.isna(i) else 1 for i in test_data['judicial_reason']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['nan_num']=(train_data==0).astype(int).sum(axis=1)\n",
    "test_data['nan_num']=(test_data==0).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delnan(df):\n",
    "    df = df.dropna(how=\"all\", axis=1)  # 删除全是空值的列\n",
    "    df = df.fillna(-99)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = delnan(train_data)\n",
    "df_test = delnan(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理-99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.replace(-99,-1,inplace=True)\n",
    "df_test.replace(-99,-1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理分类变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_col = [\"basic_1\",\n",
    "            \"basic_10\",\n",
    "            \"basic_12\",\n",
    "            \"basic_14\",\n",
    "            \"loan1_16\",\n",
    "            \"loan1_20\",\n",
    "            \"loan1_23\",\n",
    "            \"loan1_25\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转化为one-hot编码（不宜采用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(df, colnames):\n",
    "    df = pd.get_dummies(df, columns=colnames)\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\" df_train = to_one_hot(df_train, type_col)\n",
    "df_test = to_one_hot(df_test, type_col)\n",
    "for i in df_train.columns:\n",
    "    if i not in df_test.columns:\n",
    "        df_test[i] = 0\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 证据权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cal_woe(df,df_no_label,x_cols,y_col):\n",
    "    for i in x_cols:\n",
    "        x_set=pd.unique(df[i])\n",
    "        for j in x_set:\n",
    "            woe=np.mean(df[df[i]==j][y_col])\n",
    "            df[i].replace(j,woe,inplace=True)\n",
    "            df_no_label[i].replace(j,woe,inplace=True)\n",
    "    return df,df_no_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train,df_test=cal_woe(df_train,df_test,type_col,\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_col_name = list(df_train.columns)\n",
    "basic_col = [i for i in full_col_name if \"basic\" in i]\n",
    "loan_col = [i for i in full_col_name if \"loan\" in i]\n",
    "overdue_col = [i for i in full_col_name if \"overdue\" in i]\n",
    "query_col = [i for i in full_col_name if \"query\" in i]\n",
    "judicial_col=[i for i in full_col_name if \"judicial\" in i]\n",
    "\n",
    "\"\"\" for i in judicial_col:\n",
    "    df_train[i].astype(\"float\")\n",
    "    df_test[i].astype(\"float\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置哪些x作为模型自变量输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = basic_col+['is_judicial']+loan_col+query_col+overdue_col\n",
    "# x_col为模型读入的自变量列名称\n",
    "\n",
    "\n",
    "\"\"\" x_col=['loan2_3_y',\n",
    " 'loan1_6',\n",
    " 'loan1_32',\n",
    " 'loan1_31',\n",
    " 'basic_7',\n",
    " 'loan2_4_y',\n",
    " 'loan2_3',\n",
    " 'loan1_15',\n",
    " 'loan2_5_y',\n",
    " 'loan1_30',\n",
    " 'loan1_7',\n",
    " 'loan1_3',\n",
    " 'query_1',\n",
    " 'query_7',\n",
    " 'loan1_17',\n",
    " 'query_4',\n",
    " 'loan1_14',\n",
    " 'basic_1_y',\n",
    " 'loan1_21',\n",
    " 'query_1_y'] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分测试集训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn import model_selection\n",
    "\n",
    "\n",
    "X = df_train[x_col]\n",
    "Y = df_train[\"label\"]\n",
    "seed = 666\n",
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, Y, test_size=test_size, random_state=seed) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "num_round = 100\n",
    "local_test_size=3000\n",
    "\n",
    "max_depth = [3, 4, 5,6,7,8]\n",
    "eta = [0.1,0.06]\n",
    "colsample_bytree = [0.8,0.7, 0.6,0.5]\n",
    "scale_pos_weight = [1, 3, 6, 10,20, 30]\n",
    "test_size=[0.3,0.5,0.7]\n",
    "subsample=[0.7,0.6,0.5,0.4]\n",
    "par_lambda=[1,1.5,2,3]\n",
    "alpha=[0,0.2,0.4,0.6,0.8]\n",
    "min_child_weight=[1,2,3,4,5,6,10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train划分为\n",
    "* 本地测试集（固定）d_localtest\n",
    "* 训练集（固定）\n",
    "  * 评估集（每次的模型不同）dtest\n",
    "  * 真正的训练集（每次的模型不同）dtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "pred_labels = []\n",
    "local_test_labels = []\n",
    "loss_seq = []\n",
    "max_auc_loss = 0\n",
    "flag=0\n",
    "important_x_seq=[]\n",
    "\n",
    "num_seq=list(range(len(df_train)))\n",
    "np.random.shuffle(num_seq)\n",
    "\n",
    "X_local_test=df_train[x_col].iloc[num_seq[0:local_test_size]]\n",
    "Y_local_test=df_train[\"label\"].iloc[num_seq[0:local_test_size]]\n",
    "\n",
    "d_localtest=xgb.DMatrix(X_local_test,label=Y_local_test)\n",
    "\n",
    "X = df_train[x_col].iloc[num_seq[local_test_size:len(df_train)]]\n",
    "Y = df_train['label'].iloc[num_seq[local_test_size:len(df_train)]]\n",
    "x_perd = xgb.DMatrix(df_test[x_col])\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    #seed = 666\n",
    "    test_size_this_round=random.choice(test_size)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, Y, test_size=test_size_this_round)\n",
    "    # seed+=1\n",
    "\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    evallist = [(dtest, 'eval')]\n",
    "\n",
    "    param = {\n",
    "        'max_depth': random.choice(max_depth),\n",
    "        'eta': random.choice(eta),\n",
    "        'colsample_bytree': random.choice(colsample_bytree),\n",
    "        'objective': 'binary:logistic',\n",
    "        #'verbosity':0,\n",
    "        'scale_pos_weight': random.choice(scale_pos_weight),\n",
    "        'eval_metric': 'auc',\n",
    "        'subsample':random.choice(subsample),\n",
    "        'alpha':random.choice(alpha),\n",
    "        'lambda':random.choice(par_lambda),\n",
    "        'min_child_weight':random.choice(min_child_weight),\n",
    "        #'seed': 666,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0,\n",
    "        #\"enable_enable_categorical\":True,\n",
    "    }\n",
    "    \n",
    "    bst = xgb.train(param, dtrain, num_round, evals=evallist,\n",
    "                    early_stopping_rounds=5, verbose_eval=False)\n",
    "    \n",
    "    loss = float(bst.eval(dtest).split(\":\")[1])\n",
    "    train_loss = float(bst.eval(dtrain).split(\":\")[1])\n",
    "    local_test_loss = float(bst.eval(d_localtest).split(\":\")[1])\n",
    "\n",
    "    pred_label = list(bst.predict(x_perd))\n",
    "    test_label = list(bst.predict(xgb.DMatrix(X_local_test)))\n",
    "    # temp=roc_auc_score(y_test,test_label)\n",
    "    # print(test_label[0:10])\n",
    "\n",
    "\n",
    "    temp_dict={\"test_auc\":loss,\"train_auc\":train_loss,'local_test_auc':local_test_loss,'test_size':test_size_this_round}\n",
    "    temp_dict.update(param)\n",
    "\n",
    "    #info_df=info_df.append(pd.DataFrame(temp_dict))\n",
    "    if flag==0:\n",
    "        info_df=pd.DataFrame(temp_dict,index=[i])\n",
    "        flag=1\n",
    "    else:\n",
    "        info_df=pd.concat([info_df,pd.DataFrame(temp_dict,index=[i])])\n",
    "\n",
    "    if loss > max_auc_loss:\n",
    "        best_bst = copy.deepcopy(bst)\n",
    "        max_auc_loss = loss\n",
    "    \n",
    "    if False:\n",
    "        pic=xgb.plot_importance(bst,max_num_features=50)\n",
    "        temp=pic.get_ymajorticklabels()\n",
    "        important_x=[str(i).split(\"\\'\")[1] for i in temp]\n",
    "        important_x.reverse()\n",
    "        important_x_seq+=important_x\n",
    "\n",
    "    print(i+1, \":\", loss, sep=\"\")\n",
    "    #print(param)\n",
    "    #print(\"-------------------------\")\n",
    "    pred_labels.append([j for j in pred_label])\n",
    "    local_test_labels.append(copy.deepcopy(test_label))\n",
    "\n",
    "\n",
    "print('最大AUC:', max(info_df['local_test_auc']), sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.to_csv(\"info.csv\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic=xgb.plot_importance(best_bst,max_num_features=50)\n",
    "temp=pic.get_ymajorticklabels()\n",
    "important_x=[str(i).split(\"\\'\")[1] for i in temp]\n",
    "important_x.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.value_counts(important_x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic_7:年龄\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(info_df['local_test_auc'],info_df['test_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "date = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "res_num = np.argsort([-i for i in info_df['local_test_auc']])\n",
    "\n",
    "# 选择前n个结果进行平均（手动指定or在本地测试集排序选出）\n",
    "if False:\n",
    "    max_auc = -1\n",
    "    list_temp = []\n",
    "    for i in range(N):\n",
    "        list_temp.append(local_test_labels[res_num[i]])\n",
    "        temp = np.mean(list_temp, axis=0)\n",
    "        temp_auc = roc_auc_score(Y_local_test, temp)\n",
    "        print(i, temp_auc)\n",
    "        if temp_auc > max_auc:\n",
    "            max_auc = temp_auc\n",
    "            n = i\n",
    "else:\n",
    "    n = 200\n",
    "    n=min(N,n)\n",
    "\n",
    "res_num = res_num[0:n]\n",
    "\n",
    "final_pred_label = np.mean([pred_labels[i] for i in res_num], axis=0)\n",
    "#local_auc = roc_auc_score(y_test, final_pred_label)\n",
    "print(\"本地测试集前\", n, \"个auc为\", [list(info_df['local_test_auc'])[i] for i in res_num], sep=\"\")\n",
    "#print(\"平均之后AUC为\",local_auc,sep=\"\")\n",
    "\n",
    "res = pd.DataFrame({\n",
    "    'cust_id': list(df_test[\"cust_id\"]),\n",
    "    'label': final_pred_label,\n",
    "})\n",
    "res.to_csv(\"./output/result\"+date+\".csv\", encoding=\"utf-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些tips\n",
    "\n",
    "* 提升n有一定提升(如何确定n？)\n",
    "  * n=40 0.6763\n",
    "  * n=10 0.6728\n",
    "  * n=100 0.6774\n",
    "  * 太大也不好\n",
    "\n",
    "# 一些进展\n",
    "\n",
    "* 随机参数\n",
    "* XGB模型\n",
    "* 本地测试集auc较高（0.75+）\n",
    "* 部分类型变量进行了one-hot处理\n",
    "* 把细分表除了loan与主表合并\n",
    "\n",
    "# tbd\n",
    "\n",
    "* 处理细分表的loan，给出指标\n",
    "* 确定变量类型，以便对所有（or重要）分类变量进行one-hot处理\n",
    "* 新模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-12\n",
    "* 0.6824（这次我留了代码\\doge）\n",
    "* 把-99变为-1，本地有提升\n",
    "* 划出了一个本地测试集\n",
    "  * 本地测试集（固定）d_localtest\n",
    "  * 训练集（固定）\n",
    "    * 评估集（每次的模型不同）dtest\n",
    "    * 真正的训练集（每次的模型不同）dtrain\n",
    "* 添加参数nan_num（然并卵）\n",
    "* tbd\n",
    "  * 找特征\n",
    "  * 加x\n",
    "  * 分析一下参数作用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
